{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebooka137e5753f",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethancheah-arvato/image-classification/blob/main/ImageClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "_uuid": "f620b15c-2e30-49ef-8b41-2c61eac80672",
        "_cell_guid": "c1dcbea6-10b4-4f9d-8323-6d6f02334ce1",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "xLwyqMbywCC9",
        "outputId": "cc7df71a-854d-4586-caa0-9b2ecc1d6204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz'\n",
        "!unlink flowers\n",
        "!mkdir flowers && tar -xzf flower_data.tar.gz -C flowers"
      ],
      "metadata": {
        "id": "Nf0blfKPwVcy",
        "outputId": "420dc5df-b475-4b9a-ebd8-23a51a8f4ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 13:38:32--  https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.98.14, 52.217.103.30, 16.182.101.80, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.98.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344873452 (329M) [application/x-gzip]\n",
            "Saving to: ‘flower_data.tar.gz’\n",
            "\n",
            "flower_data.tar.gz  100%[===================>] 328.90M  55.3MB/s    in 6.6s    \n",
            "\n",
            "2024-08-12 13:38:39 (49.5 MB/s) - ‘flower_data.tar.gz’ saved [344873452/344873452]\n",
            "\n",
            "unlink: cannot unlink 'flowers': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/flowers'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/valid'\n",
        "test_dir = data_dir + '/test'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-12T13:20:10.363755Z",
          "iopub.execute_input": "2024-08-12T13:20:10.364583Z",
          "iopub.status.idle": "2024-08-12T13:20:10.369044Z",
          "shell.execute_reply.started": "2024-08-12T13:20:10.364521Z",
          "shell.execute_reply": "2024-08-12T13:20:10.368089Z"
        },
        "trusted": true,
        "id": "x9cZyHVIwCC_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for the training set\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomRotation(45),  # Increased rotation range\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),  # Increased jitter\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for the validation set\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for the testing set\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "# Using the image datasets and the transforms, define the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/content/cat_to_name.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)"
      ],
      "metadata": {
        "id": "Jg4GQcxozN4W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_resnet50(train_dataloader, valid_dataloader, epochs=5, print_every=40, learning_rate=0.001):\n",
        "#     # Load a pre-trained ResNet50 model\n",
        "#     model = models.resnet50(weights='DEFAULT')  # Updated to use weights argument\n",
        "\n",
        "#     # Freeze parameters to avoid backpropagation through them\n",
        "#     for param in model.parameters():\n",
        "#         param.requires_grad = False\n",
        "\n",
        "#     # Get the number of input features for the last fully connected layer\n",
        "#     num_ftrs = model.fc.in_features\n",
        "\n",
        "#     # Define a new, untrained feed-forward network as a classifier\n",
        "#     model.fc = nn.Sequential(\n",
        "#         nn.Linear(2048, 4096),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Dropout(0.6),\n",
        "#         nn.Linear(4096, 1024),\n",
        "#         nn.ReLU(),\n",
        "#         nn.Dropout(0.6),\n",
        "#         nn.Linear(1024, 102),\n",
        "#         nn.LogSoftmax(dim=1)\n",
        "#     )\n",
        "\n",
        "#     # Set the device to GPU if available\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "\n",
        "#     # Define the criterion and optimizer\n",
        "#     criterion = nn.NLLLoss()\n",
        "#     optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # Training loop\n",
        "#     steps = 0\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0\n",
        "#         for inputs, labels in train_dataloader:\n",
        "#             steps += 1\n",
        "\n",
        "#             # Move input and label tensors to the GPU\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # Forward pass\n",
        "#             logps = model(inputs)\n",
        "#             loss = criterion(logps, labels)\n",
        "\n",
        "#             # Backward pass\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             if steps % print_every == 0:\n",
        "#                 model.eval()\n",
        "#                 validation_loss = 0\n",
        "#                 accuracy = 0\n",
        "\n",
        "#                 with torch.no_grad():\n",
        "#                     for inputs, labels in valid_dataloader:\n",
        "#                         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#                         logps = model(inputs)\n",
        "#                         batch_loss = criterion(logps, labels)\n",
        "#                         validation_loss += batch_loss.item()\n",
        "\n",
        "#                         # Calculate accuracy\n",
        "#                         ps = torch.exp(logps)\n",
        "#                         top_p, top_class = ps.topk(1, dim=1)\n",
        "#                         equals = top_class == labels.view(*top_class.shape)\n",
        "#                         accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "#                 print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "#                       f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "#                       f\"Validation loss: {validation_loss/len(valid_dataloader):.3f}.. \"\n",
        "#                       f\"Validation accuracy: {accuracy/len(valid_dataloader):.3f}\")\n",
        "\n",
        "#                 running_loss = 0\n",
        "#                 model.train()\n",
        "\n",
        "\n",
        "# train_resnet50(train_dataloader, valid_dataloader, epochs=10, print_every=40, learning_rate=0.001)\n",
        "import copy\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "def train_and_evaluate_model(model, dataloaders, criterion, optimizer, device, num_epochs=10):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for inputs, labels in dataloaders['train']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logps = model(inputs)\n",
        "            loss = criterion(logps, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders['train'])\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}.. Train loss: {epoch_loss:.3f}')\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloaders['valid']:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                logps = model(inputs)\n",
        "                loss = criterion(logps, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(logps, 1)\n",
        "                corrects += torch.sum(preds == labels.data)\n",
        "                total += labels.size(0)\n",
        "\n",
        "        epoch_acc = corrects.double() / total\n",
        "        epoch_val_loss = val_loss / len(dataloaders['valid'])\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}.. Validation loss: {epoch_val_loss:.3f}.. Validation accuracy: {epoch_acc:.3f}')\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Call the training function\n",
        "model = train_and_evaluate_model(model, dataloaders, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "zXh5rS7i07-a",
        "outputId": "bee67ecc-155a-4ac5-e292-9db83bf8be2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10.. Train loss: 2.579\n",
            "Epoch 1/10.. Validation loss: 1.310.. Validation accuracy: 0.671\n",
            "Epoch 2/10.. Train loss: 2.438\n",
            "Epoch 2/10.. Validation loss: 1.230.. Validation accuracy: 0.698\n",
            "Epoch 3/10.. Train loss: 2.378\n",
            "Epoch 3/10.. Validation loss: 1.218.. Validation accuracy: 0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, test_dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = corrects.double() / total\n",
        "    avg_loss = test_loss / len(test_dataloader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.3f}')\n",
        "    print(f'Test Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    print(f'Precision: {precision:.3f}')\n",
        "    print(f'Recall: {recall:.3f}')\n",
        "    print(f'F1 Score: {f1:.3f}')\n",
        "\n",
        "    # Compute and plot confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=dataloaders['test'].dataset.classes,\n",
        "                yticklabels=dataloaders['test'].dataset.classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Call the testing function\n",
        "test_model(model, test_dataloader, criterion, device)"
      ],
      "metadata": {
        "id": "tC6u39Bi1K6b",
        "outputId": "e5851f94-3d3a-4708-cf28-6eb3f63f7ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.452\n",
            "Test Accuracy: 0.634\n"
          ]
        }
      ]
    }
  ]
}